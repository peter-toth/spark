// Licensed to Cloudera, Inc. under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  Cloudera, Inc. licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
{
  "name" : "SPARK3_ON_YARN",
  "label" : "Spark 3",
  "description" : "Apache Spark is an open source cluster computing system. This service runs Spark 3 as an application on YARN. <span class=\"error\">Before adding this service, ensure that you have installed the Spark3 binaries, which are not included in CDH.</span>",
  "version" : "3.0.7110.0",
  "compatibility" : { "cdhVersion" : { "min" : "7" } },
  "runAs" : {
    "user" : "spark",
    "group" : "spark",
    "principal" : "spark"
  },
  "inExpressWizard" : true,
  "svgIcon": "images/icon.svg",
  "parcel" : {
    "repoUrl" : "https://archive.cloudera.com/p/spark3/3.1.7270.0/parcels/",
    "requiredTags" : ["spark3", "cdh"],
    "optionalTags" : ["spark-plugin", "spark3-plugin"]
  },
  "serviceDependencies" : [
    {
      "name" : "YARN",
      "required" : "true"
    },
    {
      "name" : "HBASE",
      "required" : "false"
    },
    {
      "name" : "ATLAS",
      "required" : "false"
    }
  ],
  "dependencyExtensions" : [
    {
     "extensionId" : "yarnAuxService",
     "name" : "spark3_shuffle",
     "className" : "org.apache.spark.network.yarn.YarnShuffleService",
     "type" : "classAndConfigs",
     "configs" : [
       {
         "key" : "spark3.shuffle.service.port",
         "value" : "${spark_shuffle_service_port}"
       },
       {
         "key" : "yarn.nodemanager.aux-services.spark3_shuffle.classpath",
         "value" : "${spark3_parcel_dir}/lib/spark3/yarn/spark-yarn-shuffle.jar"
       },
       {
         "key" : "spark3.authenticate",
         "value" : "${spark_authenticate}"
       }
     ]
    },
    {
      "type" : "clusterInfo",
      "extensionId": "cluster_info"
    }
  ],
  "hdfsDirs" : [
    {
      "name" : "CreateSparkUserDirCommand",
      "label" : "Create Spark User Dir",
      "description" : "Creates the Spark user directory in HDFS.",
      "directoryDescription" : "Spark HDFS user directory",
      "path" : "/user/${principal}",
      "permissions" : "0751"
    },
    {
      "name" : "CreateSparkHistoryDirCommand",
      "label" : "Create Spark History Log Dir",
      "description" : "Creates the directory in HDFS where application history will be stored.",
      "directoryDescription" : "Spark Application History directory",
      "path" : "${spark_history_log_dir}",
      "permissions" : "1777"
    },
    {
      "name" : "CreateSparkDriverLogDirCommand",
      "label" : "Create Spark Driver Log Dir",
      "description" : "Creates the directory in HDFS where driver logs in client mode will be stored.",
      "directoryDescription" : "Spark Driver Logs directory",
      "path" : "${spark_driver_log_dfs_dir}",
      "permissions" : "1777"
    }
  ],
  "serviceInit" : {
    "preStartSteps" : [
      {
        "commandName" : "CreateSparkUserDirCommand"
      },
      {
        "commandName" : "CreateSparkHistoryDirCommand"
      },
      {
        "commandName" : "CreateSparkDriverLogDirCommand"
      }
    ]
  },
  "parameters" : [
    {
      "name" : "spark3_parcel_dir",
      "label" : "Spark3 Parcel Location",
      "description" : "The location of Spark3 parcel. This should be kept synch with `parcel_dir` and set to the value '${parcel_dir}/SPARK3'.",
      "configName" : "spark3.parcel.dir",
      "default" : "/opt/cloudera/parcels/SPARK3",
      "type" : "path",
      "pathType" : "serviceSpecific",
      "required" : "true"
    },
    {
      "name" : "spark_history_log_dir",
      "label" : "Spark History Location (HDFS)",
      "description" : "The location of Spark application history logs in HDFS. Changing this value will not move existing logs to the new location.",
      "configName" : "spark.eventLog.dir",
      "default" : "/user/spark/spark3ApplicationHistory",
      "type" : "path",
      "pathType" : "serviceSpecific",
      "required" : "true",
      "translateToBaseHdfs" : "true"
    },
    {
      "name" : "spark_shuffle_service_port",
      "label" : "Spark Shuffle Service Port",
      "description" : "The port the Spark Shuffle Service listens for fetch requests. If using SPARK_ON_YARN service (i.e. Spark 2 based service), ensure a unique port is given here.",
      "configName" : "spark.shuffle.service.port",
      "default" : 7447,
      "type" : "port",
      "required" : "true"
    },
    {
      "name" : "spark_authenticate",
      "label" : "Spark Authentication",
      "description" : "Enable whether the Spark communication protocols do authentication using a shared secret. If using SPARK_ON_YARN service (i.e. Spark 1 based service), ensure that value of this property is the same in both services.",
      "configName" : "spark.authenticate",
      "required" : "true",
      "type" : "boolean",
      "default" : false,
      "configurableInWizard": true
    },
    {
      "name" : "spark_driver_log_dfs_dir",
      "label" : "Spark Driver Log Location (HDFS)",
      "description" : "The location of Spark driver logs in HDFS when Spark application runs in client mode. Changing this value will not move existing logs to the new location.",
      "configName" : "spark.driver.log.dfsDir",
      "default" : "/user/spark/driver3Logs",
      "type" : "path",
      "pathType" : "serviceSpecific",
      "required" : "true"
    },
    {
      "name" : "spark_driver_log_persist_to_dfs",
      "label" : "Persist Driver Logs to Dfs",
      "description" : "If enabled, driver logs in YARN client mode will be persisted to the configured Spark Driver Log Location (HDFS)",
      "configName" : "spark.driver.log.persistToDfs.enabled",
      "default" : "true",
      "type" : "boolean",
      "required" : "true"
    }
  ],
  "rolesWithExternalLinks" : ["SPARK3_YARN_HISTORY_SERVER"],
  "roles" : [
    {
      "name" : "SPARK3_YARN_HISTORY_SERVER",
      "label" : "History Server",
      "pluralLabel" : "History Servers",
      "jvmBased": true,
      "startRunner" : {
        "program" : "scripts/control.sh",
        "args" : [ "start_history_server" ],
        "environmentVariables" : {
          "HISTORY_LOG_DIR" : "${spark_history_log_dir}",
          "SPARK_DAEMON_MEMORY" : "${history_server_max_heapsize}",
          "ENABLE_SPNEGO" : "${history_server_spnego_enabled}",
          "YARN_PROXY_REDIRECT" : "true",
          "KEYSTORE_PASSWORD" : "${ssl_server_keystore_password}",
          "ENABLE_LOCAL_STORAGE" : "${enable_local_storage}"
        }
      },
      "kerberosPrincipals" : [
        {
          "name" : "SPARK_PRINCIPAL",
          "primary" : "${principal}",
          "instance" : "${host}"
        },
        {
          "name" : "SPNEGO_PRINCIPAL",
          "primary" : "HTTP",
          "instance" : "${host}"
        }
      ],
      "externalLink" : {
        "name" : "history_server_web_ui",
        "label" : "History Server Web UI",
        "url" : "http://${host}:${history_server_web_port}",
        "secureUrl" : "https://${host}:${ssl_server_port}"
      },
      "topology" : { "minInstances" : 1, "maxInstances" : 1 },
      "logging" : {
        "configFilename" : "spark3-conf/log4j.properties",
        "dir" : "/var/log/spark3",
        "filename" : "spark3-history-server-${host}.log",
        "modifiable" : true,
        "loggingType" : "log4j"
      },
      "parameters" : [
        {
          "name" : "history_server_web_port",
          "label" : "History Server WebUI Port",
          "configName" : "spark.history.ui.port",
          "description" : "The port of the history server WebUI",
          "required" : "true",
          "type" : "port",
          "default" : 18089
        },
        {
          "name" : "history_server_retained_apps",
          "label" : "Retained App Count",
          "configName" : "spark.history.retainedApplications",
          "description" : "Max number of application UIs to keep in the History Server's memory. All applications will still be available, but may take longer to load if they're not in memory.",
          "required" : "false",
          "type" : "long",
          "min" : 1,
          "default" : 50
        },
        {
          "name" : "history_server_fs_poll_interval",
          "label" : "HDFS Polling Interval",
          "configName" : "spark.history.fs.update.interval.seconds",
          "description" : "How often to poll HDFS for new applications.",
          "required" : "false",
          "type" : "long",
          "unit" : "seconds",
          "default" : 10
        },
        {
          "name" : "history_server_max_heapsize",
          "label" : "Java Heap Size of History Server in Bytes",
          "description" : "Maximum size for the Java process heap memory. Passed to Java -Xmx. Measured in bytes.",
          "required" : "true",
          "type" : "memory",
          "unit" : "bytes",
          "min" : 67108864,
          "default" : 536870912
        },
        {
          "name" : "event_log_cleaner_enabled",
          "label" : "Enable Event Log Cleaner",
          "description" : "Specifies whether the History Server should periodically clean up event logs from storage.",
          "configName" : "spark.history.fs.cleaner.enabled",
          "required" : "false",
          "type" : "boolean",
          "default" : true
        },
        {
          "name" : "event_log_cleaner_interval",
          "label" : "Event Log Cleaner Interval",
          "description" : "How often the History Server will clean up event log files.",
          "configName" : "spark.history.fs.cleaner.interval",
          "required" : "false",
          "type" : "long",
          "unit" : "seconds",
          "default" : 86400
        },
        {
          "name" : "event_log_cleaner_max_age",
          "label" : "Maximum Event Log Age",
          "description" : "Specifies the maximum age of the event logs.",
          "configName" : "spark.history.fs.cleaner.maxAge",
          "required" : "false",
          "type" : "long",
          "unit" : "seconds",
          "default" : 604800
        },
        {
          "name" : "history_server_spnego_enabled",
          "label" : "Enable User Authentication",
          "description" : "Enables user authentication using SPNEGO (requires Kerberos), and enables access control to application history data.",
          "required" : "false",
          "type" : "boolean",
          "default" : false
        },
        {
          "name" : "history_server_admin_users",
          "label" : "Admin Users",
          "description" : "Comma-separated list of users who can view all applications when authentication is enabled.",
          "configName" : "spark.history.ui.admin.acls",
          "default" : "knox",
          "type" : "string",
          "required" : "false"
        },
        {
          "name" : "ssl_server_port",
          "label" : "TLS/SSL Port Number",
          "description" : "Port where to listen for TLS/SSL connections. HTTP connections will be redirected to this port when TLS/SSL is enabled.",
          "configName" : "spark.ssl.historyServer.port",
          "default" : 18489,
          "type" : "port",
          "required" : "false"
        },
        {
          "name" : "ssl_server_protocol",
          "label" : "TLS/SSL Protocol",
          "description" : "The version of the TLS/SSL  protocol to use when TLS/SSL is enabled.",
          "configName" : "spark.ssl.historyServer.protocol",
          "default" : "TLSv1.2",
          "type" : "string",
          "required" : "false"
        },
        {
          "name" : "ssl_server_algorithms",
          "label" : "Enabled SSL/TLS Algorithms",
          "description" : "A comma-separated list of algorithm names to enable when TLS/SSL is enabled. By default, all algorithms supported by the JRE are enabled.",
          "configName" : "spark.ssl.historyServer.enabledAlgorithms",
          "default" : null,
          "type" : "string",
          "required" : "false"
        },
        {
          "name" : "enable_local_storage",
          "label" : "Use Local Storage",
          "description" : "Whether to use local storage for caching application history data, which reduces memory usage and makes service restarts faster.",
          "type" : "boolean",
          "default" : false
        },
        {
          "name" : "local_storage_dir",
          "label" : "Local Storage Directory",
          "description" : "Directory where to keep local caches of application history data.",
          "configName" : "spark.history.store.path",
          "default" : "/var/lib/spark3/history",
          "type" : "path",
          "pathType" : "localDataDir",
          "mode" : "0700"
        },
        {
          "name" : "local_storage_max_usage",
          "label" : "Max Local Storage Size",
          "description" : "Approximate maximum amount of data to use in local storage for caching application history data.",
          "configName" : "spark.history.store.maxDiskUsage",
          "type" : "long",
          "unit" : "bytes",
          "default" : 10737418240
        }
      ],
      "sslServer" : {
        "keyIdentifier" : "spark_history_server",
        "enabledConfigName" : "spark.ssl.historyServer.enabled",
        "keystoreLocationConfigName" : "spark.ssl.historyServer.keyStore",
        "keystorePasswordCredentialProviderCompatible" : false,
        "keystorePasswordScriptBased" : false,
        "autoTlsMode" : "auto"
      },
      "configWriter" : {
        "generators" : [
          {
            "filename" : "spark3-conf/spark-history-server.conf",
            "configFormat" : "properties",
            "includedParams" : [
              "history_server_web_port",
              "history_server_retained_apps",
              "history_server_fs_poll_interval",
              "history_server_admin_users",
              "event_log_cleaner_enabled",
              "event_log_cleaner_interval",
              "event_log_cleaner_max_age",
              "ssl_enabled",
              "ssl_server_keystore_location",
              "ssl_server_protocol",
              "ssl_server_algorithms",
              "ssl_server_port",
              "spark_driver_log_dfs_dir",
              "local_storage_dir",
              "local_storage_max_usage"
            ],
            "additionalConfigs" : [
              {
                "key" : "spark.port.maxRetries",
                "value" : "0"
              },
              {
                "key": "spark.ssl.historyServer.keyStoreType",
                "value": "${keystore_type}"
              }
            ]
          }
        ],
        "auxConfigGenerators" : [
          {
            "filename" : "spark3-conf/spark-env.sh",
            "sourceFilename" : "aux/client/spark-env.sh"
          },
          {
            "filename" : "meta/version",
            "sourceFilename" : "meta/version"
          }
        ]
      },
      "healthAggregation" : {
        "type" : "singleton"
      },
      "commands" : [
        {
          "name" : "CleanHistoryCache",
          "label" : "Clean History Server Cache",
          "description" : "Remove all cached application history data from local storage.",
          "expectedExitCodes" : [ 0 ],
          "requiredRoleState" : "stopped",
          "commandRunner" : {
            "program" : "scripts/control.sh",
            "args" : [ "clean_history_cache", "${local_storage_dir}" ]
          }
        }
      ]
    }
  ],
  "gateway" : {
    "alternatives" : {
      "name" : "spark3-conf",
      // The priority is set to be higher than Spark standalone by default
      "priority" : 51,
      "linkRoot" : "/etc/spark3"
    },
    "parameters" : [
      {
        "name" : "spark_history_enabled",
        "label" : "Enable History",
        "description" : "Write Spark application history logs to HDFS.",
        "configName" : "spark.eventLog.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_deploy_mode",
        "label" : "Default Application Deploy Mode",
        "description" : "Which deploy mode to use by default. Can be overridden by users when launching applications.",
        "required" : "false",
        "type" : "string_enum",
        "validValues" : [ "client", "cluster" ],
        "default" : "client"
      },
      {
        "name" : "spark_data_serializer",
        "label" : "Spark Data Serializer",
        "description" : "Name of class implementing org.apache.spark.serializer.Serializer to use in Spark applications.",
        "configName" : "spark.serializer",
        "default" : "org.apache.spark.serializer.KryoSerializer",
        "type" : "string",
        "required" : "true"
      },
      {
        "name" : "spark_shuffle_service_enabled",
        "label" : "Enable Shuffle Service",
        "description" : "Enables the external shuffle service. The external shuffle service preserves shuffle files written by executors so that the executors can be deallocated without losing work. Must be enabled if Enable Dynamic Allocation is enabled.  Recommended and enabled by default.",
        "configName" : "spark.shuffle.service.enabled",
        "required" : "true",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_python_path",
        "label" : "Extra Python Path",
        "description" : "Python library paths to add to PySpark applications.",
        "required" : "false",
        "type" : "path_array",
        "pathType" : "serviceSpecific",
        "separator" : ":",
        "default" : [ ]
      },
      {
        "name" : "spark_dynamic_allocation_enabled",
        "label" : "Enable Dynamic Allocation",
        "description" : "Enable dynamic allocation of executors in Spark applications.",
        "configName" : "spark.dynamicAllocation.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_dynamic_allocation_initial_executors",
        "label" : "Initial Executor Count",
        "description" : "When dynamic allocation is enabled, number of executors to allocate when the application starts. By default, this is the same value as the minimum number of executors.",
        "configName" : "spark.dynamicAllocation.initialExecutors",
        "required" : "false",
        "type" : "long",
        "min" : 0
      },
      {
        "name" : "spark_dynamic_allocation_min_executors",
        "label" : "Minimum Executor Count",
        "description" : "When dynamic allocation is enabled, minimum number of executors to keep alive while the application is running.",
        "configName" : "spark.dynamicAllocation.minExecutors",
        "required" : "false",
        "type" : "long",
        "min" : 0,
        "default" : 0
      },
      {
        "name" : "spark_dynamic_allocation_max_executors",
        "label" : "Maximum Executor Count",
        "description" : "When dynamic allocation is enabled, maximum number of executors to allocate. By default, Spark relies on YARN to control the maximum number of executors for the application.",
        "configName" : "spark.dynamicAllocation.maxExecutors",
        "required" : "false",
        "type" : "long",
        "min" : 1
      },
      {
        "name" : "spark_dynamic_allocation_idle_timeout",
        "label" : "Executor Idle Timeout",
        "description" : "When dynamic allocation is enabled, time after which idle executors will be stopped.",
        "configName" : "spark.dynamicAllocation.executorIdleTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0,
        "default" : 60
      },
      {
        "name" : "spark_dynamic_allocation_cached_idle_timeout",
        "label" : "Caching Executor Idle Timeout",
        "description" : "When dynamic allocation is enabled, time after which idle executors with cached RDDs blocks will be stopped. By default, they're never stopped.",
        "configName" : "spark.dynamicAllocation.cachedExecutorIdleTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0
      },
      {
        "name" : "spark_dynamic_allocation_scheduler_backlog_timeout",
        "label" : "Scheduler Backlog Timeout",
        "description" : "When dynamic allocation is enabled, timeout before requesting new executors when there are backlogged tasks.",
        "configName" : "spark.dynamicAllocation.schedulerBacklogTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0,
        "default" : 1
      },
      {
        "name" : "spark_dynamic_allocation_sustained_scheduler_backlog_timeout",
        "label" : "Sustained Scheduler Backlog Timeout",
        "description" : "When dynamic allocation is enabled, timeout before requesting new executors after the initial backlog timeout has already expired. By default this is the same value as the initial backlog timeout.",
        "configName" : "spark.dynamicAllocation.sustainedSchedulerBacklogTimeout",
        "required" : "false",
        "type" : "long",
        "unit" : "seconds",
        "min" : 0
      },
      {
        "name" : "spark_gateway_shell_logging_threshold",
        "label" : "Shell Logging Threshold",
        "description" : "The minimum log level for the Spark shell.",
        "required" : "true",
        "type" : "string_enum",
        "validValues" : [ "TRACE", "DEBUG", "INFO", "WARN", "ERROR",  "FATAL" ],
        "default" : "WARN"
      },
      {
        "name" : "spark_gateway_ui_kill_enabled",
        "label" : "Enable Kill From UI",
        "description" : "Whether to allow users to kill running stages from the Spark Web UI.",
        "configName" : "spark.ui.killEnabled",
        "required" : "true",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_network_encryption_enabled",
        "label" : "Enable Network Encryption",
        "description" : "Whether to encrypt communication between Spark processes belonging to the same application. Requires authentication (spark.authenticate) to be enabled.",
        "configName" : "spark.network.crypto.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : false
      },
      {
        "name" : "spark_io_encryption_enabled",
        "label" : "Enable I/O Encryption",
        "description" : "Whether to encrypt temporary shuffle and cache files stored by Spark on the local disks.",
        "configName" : "spark.io.encryption.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : false
      },
      {
        "name" : "spark_ui_enabled",
        "label" : "Enable Spark Web UI",
        "description" : "Whether to enable the Spark Web UI on individual applications. It's recommended that the UI be disabled in secure clusters.",
        "configName" : "spark.ui.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_lineage_enabled",
        "label" : "Enable Spark Lineage",
        "description" : "Whether to enable spark lineage support. If enabled, spark lineage is sent to Atlas.",
        "configName" : "spark.lineage.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      },
      {
        "name" : "spark_optimized_s3_committers_enabled",
        "label" : "Enable Optimized S3 Committers",
        "description" : "Whether use optimized committers when writing data to S3.",
        "configName" : "spark.cloudera.s3_committers.enabled",
        "required" : "false",
        "type" : "boolean",
        "default" : true
      }
    ],
    "dependencyExtensions": [
      {
        "type" : "atlasDependency",
        "extensionId": "atlas_dependency"
      }
    ],
    "scriptRunner" : {
      "program" : "scripts/control.sh",
      "args" : [ "client" ],
      "environmentVariables" : {
        "DEPLOY_MODE" : "${spark_deploy_mode}",
        "PYTHON_PATH" : "${spark_python_path}",
        "ATLAS_KAFKA_BROKERS_LIST" : "${atlas_kafka_bootstrap_servers}",
        "ATLAS_ZK_CONNECT" : "${atlas_kafka_zk_connect}",
        "KAFKA_SECURITY_PROTOCOL": "${atlas_kafka_security_protocol}"
      }
    },
    "logging" : {
      "configFilename" : "spark3-conf/log4j.properties",
      "loggingType" : "log4j",
      "additionalConfigs" : [
        { "key" : "shell.log.level", "value" : "${spark_gateway_shell_logging_threshold}" },
        { "key" : "log4j.logger.org.spark-project.jetty", "value" : "WARN" },
        { "key" : "log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle", "value" : "ERROR" },
        { "key" : "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper", "value" : "INFO" },
        { "key" : "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter", "value" : "INFO" },
        { "key" : "log4j.logger.org.apache.parquet", "value" : "ERROR" },
        { "key" : "log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler", "value" : "FATAL" },
        { "key" : "log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry", "value" : "ERROR" }
      ]
    },
    "configWriter" : {
      "generators" : [
        {
          "filename" : "spark3-conf/spark-defaults.conf",
          "configFormat" : "properties",
          "includedParams" : [
            "spark_authenticate",
            "spark_history_enabled",
            "spark_history_log_dir",
            "spark_data_serializer",
            "spark_shuffle_service_enabled",
            "spark_shuffle_service_port",
            "spark_dynamic_allocation_enabled",
            "spark_dynamic_allocation_initial_executors",
            "spark_dynamic_allocation_min_executors",
            "spark_dynamic_allocation_max_executors",
            "spark_dynamic_allocation_idle_timeout",
            "spark_dynamic_allocation_cached_idle_timeout",
            "spark_dynamic_allocation_scheduler_backlog_timeout",
            "spark_dynamic_allocation_sustained_scheduler_backlog_timeout",
            "spark_gateway_ui_kill_enabled",
            "spark_network_encryption_enabled",
            "spark_io_encryption_enabled",
            "spark_ui_enabled",
            "spark_driver_log_dfs_dir",
            "spark_driver_log_persist_to_dfs",
            "spark_optimized_s3_committers_enabled",
            "spark_lineage_enabled"
          ]
        }
      ],
      "auxConfigGenerators" : [
        {
          "filename" : "spark3-conf/spark-env.sh",
          "sourceFilename" : "aux/client/spark-env.sh"
        },
        {
          "filename" : "meta/version",
          "sourceFilename" : "meta/version"
        }
      ],
      "peerConfigGenerators" : [
        {
          "filename" : "spark3-conf/history3.properties",
          "params" : ["history_server_web_port"],
          "roleName" : "SPARK3_YARN_HISTORY_SERVER"
        }
      ]
    }
  }
}
